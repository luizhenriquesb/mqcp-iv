---
title: "Aula 13"
author: "Luiz Henrique"
date: "2023-12-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Trabalho final

Exemplo de modelo

-   Introdução ao estudo a ser replicado

-   Descrição do banco de dados

-   Replicação dos resultados

-   Análises adicionais (checagem do modelo)

-   Enviar um arquivo com o texto, gráficos, tabelas, mas sem os códigos que usamos e outro arquivo com o código

-   Trabalho pode ser bem objetivo, não precisa ser longo (umna página de texto estaria bom)

### Capítulo 12 -- Regressão múltipla

...

#### 12.6 Matriz chapéu

#### 12.7 Multicolinearidade

Multicolinearidade existe quando uma colunas (que é um dos nossos preditores) é uma **combinação linear** de outras colunas (nossas outras variáveis). Nesses casos, não podemos calcular a matriz...(?) e, portanto, não conseguimos calcular o beta.

**Combinações não lineares** não tem problema!

##### Exemplo no R

Veja abaixo que `x2` fica como `NA`.

Por quê? Professor explicou, mas não anotei.

```{r}
n <- 100
x1 <- rnorm(n)
x2 <- x1*2 + 10
y <- x1 + rnorm(n)
eg <- lm(y ~x1 + x2)
summary(reg)
```

```{r}
x3 <- rnorm(n)
x4 <- .7*x1 + .5*x3 - 10
reg <- lm(y ~x1 + x3 + x4)
summary(reg)
```

O R não roda se tiver multicolinearidade *perfeita*.

#### 12.8 Erro padrão robusto

Raramente vemos ter homocedasticidade. O padrão é supor heterocedasticidade. Nestes casos, o que fazemos? Não podemos utilizar o erro padrão como normalmente fizemos ao longo do curso. Isso porque a variância, no caso da heterocedasticidade, não é constante. Assim, precisamos corrigir o erro padrão, para obter o **erro padrão robusto**.

```{r}
x <- c(1:8, 10, 15)
y <- c(5 + rnorm(8,sd = 1.5), 40, 65)
df <- data.frame(y=y, x=x)

df |> 
  ggplot(aes(x=x, y=y)) + geom_point()
```

```{r}
fit <- lm(y ~x)
summary(fit)
```

Na prática, devemos rodar a regressão considerando o erro padrão robusto. No R, o default do erro padrão robusto é o HC3. No Stata, o padrão é HC1. Se a amostra for grande, tanto faz, mas se for pequena, o HC3 é mais recomendado.

Padronizar os coeficientes: $x_1 = \frac{X_i - \hat{X}i}{DP(Xi)}$

#### 12.9 Análise de sensibilidade causal

Para fazer interpretação causal, devemos nos preocupar com viés de seleção. A análise de sensibilidade tem sido uma das formas de checarmos se nosso viés de variável omitida é grave ou não. A ideia dessa análise é checar qual o tamanho do viés de v. omitida... deixasse de ser significativa.

##### 12.9.1 Derivação do R-quadrado

O que é o R-quadrado (para relembrar)?

Três medidas que nos ajudam a chegar no R-quadrado:

-   $T = Y - \bar{Y}$

-   $\hat{M} = \hat{Y} - \bar{Y}$

-   ... (tem mais coisa, não anotei)

Da variância total, quanto nosso modelo está capturando (esse é o R-quadrado). Algumas pessoas interpretam o R-quadrado como o percentual... explicada pelo modelo, mas isso envolve uma ideia de causalidade, que não existe aqui.

##### Calculando o R-quadrado

```{r}
library(here)
library(data.table)
library(tidyverse)
library(sjlabelled) # pra remover labelled variables
library(haven)
library(janitor)
library(lubridate)

## dados
# https://www.latinobarometro.org/latContents.jsp
lat_bar23 <- sjlabelled::read_spss("latino_barometro/Latinobarometro_2023_Eng_Spss_v1_0.sav",
                                   drop.labels = TRUE) %>%
  mutate(S17 = as.Date(as.character(S17), "%Y%m%d")) %>%
  clean_names()
```
