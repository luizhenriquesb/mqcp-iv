---
title: "Fichamento Expositivo - Estimação (capítulo 7)"
subtitle: "Livro: Galdino, Manoel. Estimação (capítulo 7). In: _______. Introdução à Regressão para Ciências Sociais"
format: html
editor: visual
---

::: {style="text-align: justify"}
## Capítulo 7 - Estimação

```{r pacotes, message=FALSE, warning=FALSE}
# Pacotes utilizados ao longo do script
library(tidyverse)
library(knitr)
library(MASS)
```

O preditor linear ótimo com um único regressor possui:

-- Intercepto $\alpha = E[Y] - \beta * E[X]$\
-- Inclinação $\beta = cov(Y, X) / var(X)$

**O que isso significa?**

Se "considerarmos o modelo de regressão $y_i = \alpha + \alpha * x_i + e_i$ e usarmos essas fórmulas para calcular os valores de $\alpha$ e $\beta$ em uma população, obteríamos uma reta ajustada que é o melhor preditor linear".

Conforme lidamos com uma amostra *como se fosse* uma população, para facilitar as demonstrações, foi deixado de lado como funciona "a estimação de uma regressão a partir de uma amostra". Mas isso será visto agora.

## 7.1 Plug-in estimators

"Se tivermos uma amostra, e não a população, é razoável pensar que uma boa estimativa para os valores populacionais de $\alpha$ e $\beta$ são justamente essas fórmulas, calculadas para os dados amostrais".

**Plug-in estimates** é o nome dado ao estimador dos parâmetros populacionais.

É comum usar as letras gregas com "chapéu" para representar estimativas amostrais de valores populacionais ou letras latinas $b$.

## 7.2 Simulação de uma Normal Bivariada

Quando dizemos que duas variáveis aleatórias $X$ e $Y$ apresentam uma distribuição conjunta que é normal, isso significa que tanto $X$ quanto $Y$ seguem uma distribuição normal individualmente e, mesmo quando combinada linearmente (como em $aX + bY$, onde $a$ e $b$ são constantes), a combinação também segue uma distribuição normal.

Os parâmetros de uma distribuição normal bivariada são representados de forma vetorial e matricial. Ao invés de termos apenas uma média, agora temos um vetor para representar a média das nossas duas variáveis aleatórias, $X$ e $Y$.

$$
\mu = (\mu X, \mu Y)
$$

Para compreender a variação dos dados, além das variâncias individuais de $X(\sigma_X)$ e $Y(\sigma_Y)$, também é importante saber a covariância \*\*entre $X$ e $Y$ $(cov[X, Y])$\*\* e \*\*entre $Y$ e $X$ $(cov[Y, X])$\*\*. Com essas quatro informações temos uma matriz variância-covariância:

$$
\sum = \begin{bmatrix} \sigma_x & cov[Y, X] \\cov[X, Y] & \sigma_y \\\end{bmatrix}
$$

Em vez de trabalhar diretamente com a matriz de covariância, muitas vezes é mais intuitivo e conveniente usar a correlação entre as variáveis para descrever a relação entre elas em uma distribuição normal bivariada. Por isso, vamos usar a seguinte matriz:

$$
\sum = \begin{bmatrix} \sigma_{x}^{2} & p \cdot \sigma_x \cdot \sigma_y \\ p \cdot \sigma_x \cdot \sigma_y & \sigma_{y}^{2} \\\end{bmatrix}
$$

Se conhecemos os desvios-padrão de X e Y e sua correlação, podemos calcular a matriz de covariância. Isso permite uma representação mais simplificada e intuitiva dos parâmetros.

#### Definindo os parâmetros da Normal Bivariada

```{r, message=FALSE, warning=FALSE}
# Vetor de médias
vetor_media <- c(0,0) # mesmo que as médias sejam iguais, é preciso representá-las em vetores

# Definindo sigma (letra grega para o desvio padrão/variância)
sigma_x <- 2 
sigma_y <- 2

rho <- .6

# Criando a matriz (segundo parâmetro)
matriz_var_cov <- matrix(
  # Valores para o segundo parâmetro (sigma grande) da Normal Bivariada
  c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y,  sigma_y^2 ), 
  # Começa prencheendo as linhas ao invés das colunas
  byrow=T,
  # Define o número de linhas da matriz
  nrow=2
  )

# Cria a matriz e apresenta numa tabela
matriz_var_cov |> 
  knitr::kable()
```

#### Vamos, agora, fazer a simulação.

```{r, message=FALSE, warning=FALSE}
# Definindo semente
set.seed(345)

# Normal bivariada
norm_bivariada <- MASS::mvrnorm(
  # Gera uma amostra de 10 mil observações
  n = 10000,
  # Primeiro parâmetro
  mu = vetor_media, 
  # Segundo parâmetro
  Sigma  = matriz_var_cov)

# Transformando em uma tabela
bivariada_tibble1 <- as_tibble(
  norm_bivariada,
  # Repara o nomes inválidos das colunas 
  .name_repair = "universal") |> 
  # Renomeando as colunas
  rename(x = '...1', y = '...2')
```

#### Plotando a variável X em um gráfico de densidade

```{r}
# Plotando em um gráfico de densidade
bivariada_tibble1 |> 
  ggplot() +
  aes(x) +
  geom_density()
```

#### Plotando a variável Y

```{r}
# Plotando em um gráfico de densidade
bivariada_tibble1 |> 
  ggplot() +
  aes(y) +
  geom_density()

```

#### Visualizando X e Y em um gráfico 2D

```{r}
# Plotando em um gráfico de densidade 2D
bivariada_tibble1 |> 
  ggplot() +
  aes(x, y) +
  geom_density2d()
```

#### Visualizando X e Y em um gráfico de pontos

```{r}
# Plotando em um gráfico de pontos
bivariada_tibble1 |> 
  ggplot() +
  aes(x, y) +
  geom_point()
```

#### Fazendo o modelo regressão

```{r}
# Criando objeto que guarda o modelo
reg1 <- lm(y ~ x, data = bivariada_tibble1)

summary(reg1) # Mais para frente do curso, vamos aprender a interpretar os resultados
```

### Construindo outros modelos

Em seguida, no livro, o autor cria outros modelos com valores diferentes para os parâmetros e plota as representações gráficas. Aqui, vou só reproduzir os modelos e apresentar a tabela comparando os resultados entre eles.

#### Modelo 2

```{r, message=FALSE, warning=FALSE}
# vetor de médias
vetor_media <- c(0,0)

# cov menor, dp o mesmo
sigma_x <- 2
sigma_y <- 2

rho <- .3

# Criando a matriz (segundo parâmetro)
matriz_var_cov <- matrix(c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y,  sigma_y^2 ), byrow=T, nrow=2)

# Normal bivariada
norm_bivariada <- MASS::mvrnorm(n = 10000, 
                                # Primeiro parâmetro
                                mu = vetor_media,
                                # Segundo parâmetro
                                Sigma  = matriz_var_cov)

# Transformando em uma tabela
bivariada_tibble2 <- as_tibble(norm_bivariada, .name_repair = "universal") |>
  # Renomeando as colunas
  rename(x = '...1', 
         y = '...2')

# Criando o modelo 2
reg2 <- lm( y ~x , data = bivariada_tibble2)
```

#### Modelo 3

```{r, message=FALSE, warning=FALSE}
# cov igual, dp de x maior
sigma_x <- 4

rho <- .3

# Criando matriz
matriz_var_cov <- matrix(
  c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y,  sigma_y^2 ), 
  byrow=T, 
  nrow=2)

# Normal Bivariada
norm_bivariada <- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma  = matriz_var_cov)

# Transformando em uma tabela
bivariada_tibble3 <- as_tibble(norm_bivariada, .name_repair = "universal") |>
  rename(x = '...1', 
         y = '...2')

# Criando o modelo 3
reg3 <- lm( y ~x , data = bivariada_tibble3)
```

#### Modelo 4

```{r, message=FALSE, warning=FALSE}
# cov maor, dp de x maior
sigma_x <- 4
sigma_y <- 2

rho <- .15

# Criando matriz
matriz_var_cov <- matrix(
  c(sigma_x^2, rho*sigma_x*sigma_y, rho*sigma_x*sigma_y,  sigma_y^2 ), 
  byrow=T, 
  nrow=2)

# Normal bivariada
norm_bivariada <- MASS::mvrnorm(n = 10000, mu = vetor_media, Sigma  = matriz_var_cov)

# Transformando em uma tabela
bivariada_tibble4 <- as_tibble(norm_bivariada, .name_repair = "universal") |>
  rename(x = '...1', 
         y = '...2')

# Criando o modelo 4
reg4 <- lm( y ~x , data = bivariada_tibble4)
```

### Tabela para comparação dos resultados

```{r}
library("stargazer")

stargazer::stargazer(list(reg1, reg2, reg3, reg4), 
                     type =  "html", 
                     style = "ajps",
                     title = "Regressão linear", 
                     omit.stat = "f",
                     column.labels=c("cov = 2.4, s_x = 2", "cov = 1.2, s_x = 2", "cov = 2.4, s_x = 4", "cov = 1.2, s_x = 4" )
                     )
```

## 7.3 Suposições amostrais de OLS

Quando usamos dados amostrais para fazer inferências ou estimar parâmetros, é comum assumir certas condições sobre a natureza dos dados.

A suposição de **IID (independentes e identicamente distribuídos)** é um pressuposto fundamental. Esse pressuposto significa que em uma amostra de duas varáveis $X$ e $Y$, a suposição de IID significa que os pares de observações $x_i, y_i$ e $x_j, y_j$ são independentes um do outro quando $i \neq j$ . Ou seja, as observações não têm relação direta entre si e não influenciam umas às outras. Além disso, elas são assumidas como tendo a mesma distribuição de probabilidade.

Essa suposição é crucial para certos cálculos, particularmente para calcular a variância dos estimadores. Ao calcular os coeficientes de um modelo de regressão linear usando mínimos quadrados ordinários, a variância dos estimadores desses coeficientes depende dessa suposição de i.i.d. Se essa condição não for atendida, as estimativas dos parâmetros podem ser enviesadas ou imprecisas.

No entanto, é importante ressaltar que a suposição de i.i.d. é idealizada e nem sempre é plenamente verdadeira na prática. Em situações onde há dependência entre as observações, como amostras agrupadas de uma mesma fonte (alunos de uma mesma escola, por exemplo), existem métodos estatísticos específicos para lidar com essa dependência, como modelos de efeitos mistos ou modelos de análise de covariância (ANCOVA).

## 7.4 Modelo de média amostral

Esse trecho aborda o modelo de regressão mais simples, onde não há preditores além da constante. Nesse modelo, a relação entre a variável dependente $Y$ e o erro $e$ é expressa como $Y = \mu + e$. Aqui, $\mu$ representa o intercepto ou a média de $Y$ na população e $e$ é o termo de erro.

O estimador de mínimos quadrados para $\mu$, que é simbolizado por $\hat{\mu }$, é simplesmente a média amostral ($\bar{y}$, ou seja, a média dos valores observados da variável $Y$ na amostra.

Ao calcular a média do estimador de mínimos quadrados ($\bar{y}$) e sua variância, é possível avaliar a precisão do estimador. Quando a média do estimador de mínimos quadrados é igual à média populacional (nesse caso, $\bar{u}$), dizemos que o estimador é **não-viesado**.

Em outras palavras, um estimador não-viesado é aquele que, em média, produz estimativas que estão próximas do valor real do parâmetro que está sendo estimado, neste caso, a média populacional $\mu$.

## 7.5 Modelo de Regressão Linear com preditores

Vamos agora fazer o mesmo cálculo com um preditor. Vamos, também, "assumir que o modelo de regressão linear é uma boa aproximação para CEF".

1.  O modelo linear $y_i = \alpha + \beta*x_i + e_i$ é adequado
2.  $E[e|X] = 0$, isto é, o erro é não correlacionado com x

No restante deste trecho, o autor demonstra a equação do estimador $\hat{\beta}$, demonstra que o estimador é não-viesado, faz o cáluclo da variância do estimador $\hat{\beta}$ e mostra como calcular o erro padrão do estimador $\hat{\beta}$.

## 7.6 Estimando um modelo de regressão real

Vamos começar com um exemplo, importando dados do site Base de Dados. Para tanto, precisaremos criar um projeto no Google cloud, conforme os passos aqui: https://basedosdados.github.io/mais/access_data_packages/.

Depois, só escolher quais dos dados iremos olhar https://basedosdados.org/.

```{r}
# instalando a biblioteca
# install.packages('basedosdados')

# carregando a biblioteca na sessão
library(basedosdados)

# para importar os dadosdiretamente no R, precisamos criar um id para acessar a base de dados
set_billing_id("mqcp-iv")

# checando se deu certo
get_billing_id()
```

```{r}
query <- "SELECT * FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = 'AL' and id_municipio = '2704302' and ano = 2019" 

escola <- read_sql(query)


# query <- "SELECT count(*) as contagem, id_municipio FROM `basedosdados.br_inep_censo_escolar.escola` where sigla_uf = 'AL' and ano = 2019 group by id_municipio" 
# escola <- read_sql(query)

query <- "SELECT * FROM `basedosdados.br_inep_censo_escolar.dicionario`"

dicionario <- read_sql(query)
```

```{r}
query <- "SELECT * FROM `basedosdados.br_inep_censo_escolar.matricula` where sigla_uf = 'AL' and id_municipio = '2704302' and ano = 2019"

aluno <- read_sql(query)
```

```{r}
# dicionário de gênero e raça
# dicionario |>
#   dplyr::filter(id_tabela == "matricula", nome_coluna %in% c("sexo", "raca_cor"))

aluno_gen <- aluno |>
  dplyr::filter(regular == 1) |>
  group_by(id_escola, sexo) |>
  summarise(num_aluno_gen = n()) |>
  mutate(sexo = gsub("1", "Male", sexo),
         sexo = gsub("2", "Female", sexo)) |>
  mutate(total = sum(num_aluno_gen),
            percent = num_aluno_gen/total) |>
  filter(sexo == "Female") |>
  rename(percent_female = percent) |>
  dplyr::select(id_escola, percent_female)
         
aluno_raca <- aluno |>
  dplyr::filter(regular == 1) |>
  group_by(id_escola, raca_cor) |>
  summarise(num_aluno_raca = n()) |>
  mutate(raca_cor = gsub("1", "Branca", raca_cor),
         raca_cor = gsub("2", "Preta", raca_cor),
         raca_cor = gsub("3", "Parda", raca_cor)) |>
  mutate(total = sum(num_aluno_raca),
         percent = num_aluno_raca/total) |>
  filter(raca_cor %in% c("Branca", "0", "Preta", "Parda")) |>
  mutate(raca_cor = gsub("0","não_declarado", raca_cor)) |>
  dplyr::select(id_escola, raca_cor, percent) |>
  pivot_wider(names_from = raca_cor , values_from = percent)

aluno_escola <- aluno_gen |>
  inner_join(aluno_raca,  by = "id_escola") |>
  inner_join(escola, by = "id_escola")

aluno_escola_reg <- aluno_escola |>
  ungroup() |>
  mutate(negra = Preta + Parda) |>
  dplyr::select(negra, percent_female, tipo_localizacao, agua_potavel, esgoto_rede_publica, 
         lixo_servico_coleta, area_verde, biblioteca, quantidade_profissional_psicologo) |>
  filter(across(everything(), ~!is.na(.))) |>
  mutate_if(bit64::is.integer64, as.factor)

reg <- lm(negra ~  percent_female + tipo_localizacao + agua_potavel  + esgoto_rede_publica + lixo_servico_coleta + area_verde + biblioteca + quantidade_profissional_psicologo, data= aluno_escola_reg)

summary(reg)
```

## 7.7 Limitações de OLS

As suposições padrão nos mínimos quadrados ordinários (MQO) nos oferecem certa confiança no não-viés dos estimadores e na possibilidade de calcular a variância amostral deles. No entanto, para realizar inferências mais avançadas, como calcular intervalos de confiança, realizar testes de hipóteses ou obter valores-p, precisamos de suposições adicionais que vão além das suposições básicas do modelo linear.

## 7.8 Distrações

Quando nós imprimimos o resultado de uma regressão no R, além dos coeficientes e o erro padrão, há uma série de outras informações apresentadas. Muitas delas são apenas distrações.

### 7.8.1 R-quadrado

### 7.8.2 R-quadrado ajustado

### 7.8.3 Limitações do R-quadrado

### 7.8.4 Para quê serve?
:::
